<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=BnIqpcFAcReH0Tw3JLCcnA);ul.lst-kix_3hwy46wqzz7a-1{list-style-type:none}ul.lst-kix_3hwy46wqzz7a-0{list-style-type:none}ul.lst-kix_3hwy46wqzz7a-3{list-style-type:none}ul.lst-kix_o9wihv5pvi6d-8{list-style-type:none}ul.lst-kix_3hwy46wqzz7a-2{list-style-type:none}ul.lst-kix_o9wihv5pvi6d-7{list-style-type:none}ul.lst-kix_o9wihv5pvi6d-6{list-style-type:none}ul.lst-kix_o9wihv5pvi6d-5{list-style-type:none}ul.lst-kix_o9wihv5pvi6d-4{list-style-type:none}ul.lst-kix_o9wihv5pvi6d-3{list-style-type:none}ul.lst-kix_o9wihv5pvi6d-2{list-style-type:none}ul.lst-kix_o9wihv5pvi6d-1{list-style-type:none}ul.lst-kix_o9wihv5pvi6d-0{list-style-type:none}.lst-kix_o9wihv5pvi6d-8>li:before{content:"\0025a0   "}.lst-kix_o9wihv5pvi6d-7>li:before{content:"\0025cb   "}.lst-kix_o9wihv5pvi6d-6>li:before{content:"\0025cf   "}.lst-kix_o9wihv5pvi6d-3>li:before{content:"\0025cf   "}.lst-kix_3hwy46wqzz7a-4>li:before{content:"\0025cb   "}.lst-kix_3hwy46wqzz7a-5>li:before{content:"\0025a0   "}.lst-kix_o9wihv5pvi6d-1>li:before{content:"\0025cb   "}.lst-kix_o9wihv5pvi6d-5>li:before{content:"\0025a0   "}.lst-kix_3hwy46wqzz7a-2>li:before{content:"\0025a0   "}.lst-kix_3hwy46wqzz7a-6>li:before{content:"\0025cf   "}.lst-kix_o9wihv5pvi6d-0>li:before{content:"\0025cf   "}.lst-kix_o9wihv5pvi6d-4>li:before{content:"\0025cb   "}.lst-kix_3hwy46wqzz7a-0>li:before{content:"\0025cf   "}.lst-kix_3hwy46wqzz7a-1>li:before{content:"\0025cb   "}.lst-kix_3hwy46wqzz7a-8>li:before{content:"\0025a0   "}.lst-kix_3hwy46wqzz7a-7>li:before{content:"\0025cb   "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_o9wihv5pvi6d-2>li:before{content:"\0025a0   "}ul.lst-kix_3hwy46wqzz7a-8{list-style-type:none}ul.lst-kix_3hwy46wqzz7a-5{list-style-type:none}.lst-kix_3hwy46wqzz7a-3>li:before{content:"\0025cf   "}ul.lst-kix_3hwy46wqzz7a-4{list-style-type:none}ul.lst-kix_3hwy46wqzz7a-7{list-style-type:none}ul.lst-kix_3hwy46wqzz7a-6{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c23{border-right-style:solid;padding-top:0pt;border-top-width:0pt;border-right-width:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.75;border-left-width:0pt;border-top-style:solid;background-color:#ffffff;border-left-style:solid;border-bottom-width:0pt;border-bottom-style:solid;text-align:left;padding-right:0pt}.c39{border-right-style:solid;padding-top:0pt;border-top-width:0pt;border-right-width:0pt;padding-left:0pt;padding-bottom:9pt;line-height:1.74545;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;border-bottom-style:solid;text-align:left;padding-right:0pt}.c12{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:620.2pt;border-top-color:#000000;border-bottom-style:solid}.c22{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:318.8pt;border-top-color:#000000;border-bottom-style:solid}.c26{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:1726.5pt;border-top-color:#000000;border-bottom-style:solid}.c5{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:301.5pt;border-top-color:#000000;border-bottom-style:solid}.c7{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:405.8pt;border-top-color:#000000;border-bottom-style:solid}.c2{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:1320.8pt;border-top-color:#000000;border-bottom-style:solid}.c6{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:92.2pt;border-top-color:#000000;border-bottom-style:solid}.c3{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:312.8pt;border-top-color:#000000;border-bottom-style:solid}.c20{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:608.2pt;border-top-color:#000000;border-bottom-style:solid}.c15{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:295.5pt;border-top-color:#000000;border-bottom-style:solid}.c18{background-color:#ffffff;color:#202124;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Arial";font-style:normal}.c31{background-color:#ffffff;color:#292929;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Georgia";font-style:normal}.c13{background-color:#fffffe;padding-top:0pt;padding-bottom:0pt;line-height:1.3571428571428572;orphans:2;widows:2;text-align:left;height:11pt}.c41{-webkit-text-decoration-skip:none;color:#1155cc;font-weight:700;text-decoration:underline;text-decoration-skip-ink:none;font-size:9pt;font-family:"Georgia"}.c19{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c37{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Roboto";font-style:normal}.c14{background-color:#fffffe;padding-top:0pt;padding-bottom:0pt;line-height:1.3571428571428572;orphans:2;widows:2;text-align:left}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c1{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c40{color:#292929;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Georgia";font-style:normal}.c17{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c8{background-color:#ffffff;padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left;height:11pt}.c4{background-color:#ffffff;padding-top:0pt;padding-bottom:9pt;line-height:1.75;text-align:left;height:11pt}.c24{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c16{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c9{background-color:#ffffff;padding-top:0pt;padding-bottom:9pt;line-height:1.4545454545454546;text-align:left}.c27{background-color:#ffffff;padding-top:0pt;padding-bottom:0pt;line-height:1.2;text-align:left}.c34{background-color:#ffffff;padding-top:0pt;padding-bottom:2pt;line-height:1.2;text-align:left}.c28{background-color:#ffffff;font-size:15pt;color:#202124;font-weight:700}.c33{margin-left:auto;border-spacing:0;border-collapse:collapse;margin-right:auto}.c25{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c42{font-size:15pt;color:#040c28;font-weight:700}.c32{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c10{margin-left:36pt;padding-left:0pt}.c30{font-size:12pt;font-weight:700}.c29{padding:0;margin:0}.c36{color:inherit;text-decoration:inherit}.c35{font-size:17pt;font-weight:700}.c38{background-color:#ffffff}.c21{height:11pt}.c11{height:26.5pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c32 doc-content"><p class="c16"><span class="c1">Compare three different Yolo Versions V3, V5, and the latest version:</span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><a id="t.d3a11a1100b5e1b1f9675de0bf40f5f25098d411"></a><a id="t.0"></a><table class="c33"><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c9"><span class="c1">Models</span></p></td><td class="c20" colspan="2" rowspan="1"><p class="c9"><span class="c1">YOLOv3</span></p><p class="c21 c25"><span class="c1"></span></p></td><td class="c12" colspan="2" rowspan="1"><p class="c9"><span class="c1">YOLOv5</span></p><p class="c17 c21"><span class="c1"></span></p><p class="c25 c21"><span class="c1"></span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c1">Latest YOLOv.</span></p></td></tr><tr class="c11"><td class="c26" colspan="6" rowspan="1"><p class="c9"><span class="c1">[5 pt] Comparison between these Models.</span></p></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c9"><span class="c1">Deploying year</span></p></td><td class="c20" colspan="2" rowspan="1"><p class="c9"><span class="c1">2018</span></p></td><td class="c12" colspan="2" rowspan="1"><p class="c9"><span class="c1">2020</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c1">2023</span></p></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c9"><span class="c1">Architecture</span></p></td><td class="c20" colspan="2" rowspan="1"><p class="c9"><span class="c1">CNN</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 503.00px; height: 412.66px;"><img alt="" src="images/image2.png" style="width: 503.00px; height: 412.66px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 998.00px; height: 494.00px;"><img alt="" src="images/image11.png" style="width: 998.00px; height: 494.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c1">YOLOv3 (You Only Look Once version 3) is a neural network-based object detection algorithm. It improves upon the previous version, YOLOv2, by using a few different techniques, including the use of a &quot;feature pyramid network&quot; to help detect objects at different scales, and the use of &quot;multi-scale prediction&quot; to improve detection accuracy.</span></p><p class="c9 c21"><span class="c1"></span></p><p class="c9"><span class="c1">The YOLOv3 architecture consists of a backbone network based on a darknet-53 architecture, which is made up of 53 convolutional layers. This backbone network is then followed by several detection layers that perform object detection at different scales.</span></p><p class="c9 c21"><span class="c1"></span></p><p class="c9"><span class="c1">At a high level, the YOLOv3 architecture works by dividing the input image into a grid of cells, and then predicting bounding boxes and class probabilities for each cell. The bounding boxes are predicted using a technique called &quot;anchor boxes&quot;, where a set of predefined anchor boxes are used to predict the size and aspect ratio of the object in each cell. The class probabilities are predicted using a softmax function.</span></p><p class="c9 c21"><span class="c1"></span></p><p class="c9"><span class="c1">YOLOv3 also uses several techniques to improve the accuracy and speed of object detection. One such technique is &quot;multi-scale training&quot;, where the network is trained on images of different resolutions. Another technique is &quot;swish activation&quot;, which is a more complex activation function that has been shown to improve accuracy compared to other activation functions like ReLU.</span></p></td><td class="c12" colspan="2" rowspan="1"><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 594.68px; height: 444.18px;"><img alt="" src="images/image9.png" style="width: 594.68px; height: 444.18px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 2048.00px; height: 1173.00px;"><img alt="" src="images/image1.png" style="width: 2048.00px; height: 1173.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c1">It is based on a one-stage object detection approach, where the detection and classification tasks are performed in a single pass through the network. The architecture is a fully convolutional neural network with a backbone based on CSPNet (cross-stage partial network) and SPP (Spatial Pyramid Pooling) blocks. It is designed to be lightweight and fast, while maintaining high accuracy.</span></p><p class="c9 c21"><span class="c1"></span></p><p class="c9"><span class="c1">The CSPNet architecture is a variation of the popular ResNet architecture that utilizes cross-stage feature aggregation to reduce the number of parameters and increase the efficiency of the network. The SPP block is used to pool features at multiple scales and resolutions, allowing the network to capture features at different levels of abstraction.</span></p><p class="c9 c21"><span class="c1"></span></p><p class="c9"><span class="c1">YOLOv5 uses a pyramid feature hierarchy approach with three prediction scales (P3, P4, P5) to capture objects of different sizes. The network outputs bounding box predictions, objectness scores, and class probabilities for each anchor box at each prediction scale.</span></p><p class="c9 c21"><span class="c1"></span></p><p class="c9"><span class="c1">Overall, the YOLOv5 architecture is designed to be efficient, fast, and accurate, making it well-suited for real-time object detection applications.</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1956.00px; height: 2048.00px;"><img alt="" src="images/image7.png" style="width: 1956.00px; height: 2048.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c34"><span class="c1">Prediction scales</span></p></td><td class="c20" colspan="2" rowspan="1"><ul class="c29 lst-kix_o9wihv5pvi6d-0 start"><li class="c9 c10 li-bullet-0"><span class="c1">13 x 13 feature map (detects large objects)</span></li><li class="c9 c10 li-bullet-0"><span class="c1">26 x 26 feature map (detects medium-sized objects)</span></li><li class="c9 c10 li-bullet-0"><span class="c1">52 x 52 feature map (detects small objects)</span></li></ul></td><td class="c12" colspan="2" rowspan="1"><ul class="c29 lst-kix_3hwy46wqzz7a-0 start"><li class="c9 c10 li-bullet-0"><span class="c1">19 x 19 feature map (detects very large objects)</span></li><li class="c9 c10 li-bullet-0"><span class="c1">38x 38feature map (detects large objects)</span></li><li class="c9 c10 li-bullet-0"><span class="c1">76x 76feature map (detects medium-sized objects)</span></li><li class="c9 c10 li-bullet-0"><span class="c1">152x 152feature map (detects small objects) &ldquo;New&rdquo;</span></li></ul></td><td class="c7" colspan="1" rowspan="1"><ul class="c29 lst-kix_3hwy46wqzz7a-0"><li class="c9 c10 li-bullet-0"><span class="c1">20 x 20 feature map</span></li><li class="c9 c10 li-bullet-0"><span class="c1">40 x 40 feature map</span></li><li class="c9 c10 li-bullet-0"><span class="c1">80 x 80 feature map</span></li></ul></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c34"><span class="c1">&nbsp;Number of Anchors</span></p></td><td class="c20" colspan="2" rowspan="1"><p class="c9"><span class="c1">YOLOv3 uses nine anchors per prediction scale</span></p></td><td class="c12" colspan="2" rowspan="1"><p class="c9"><span class="c1">YOLOv5 uses three to five anchors per scale. This reduces the number of parameters and helps to improve speed and accuracy.</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c38 c40">uses anchor-free detection </span></p><p class="c9"><span class="c31">Ref:</span></p><p class="c9"><span class="c38 c41"><a class="c36" href="https://www.google.com/url?q=https://medium.com/mlearning-ai/yolo-v8-the-real-state-of-the-art-eda6c86a1b90&amp;sa=D&amp;source=editors&amp;ust=1707062410143494&amp;usg=AOvVaw2rGoPbGAByg9eH-OYd6hyv">https://medium.com/mlearning-ai/yolo-v8-the-real-state-of-the-art-eda6c86a1b90</a></span></p></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c34"><span class="c30">Number of </span><span class="c30 c38">params</span></p></td><td class="c20" colspan="2" rowspan="1"><p class="c9"><span class="c1">YOLOv3 has 61 million parameters.</span></p></td><td class="c12" colspan="2" rowspan="1"><p class="c9"><span class="c1">The number of parameters in YOLOv5 is significantly lower than that of YOLOv3. For example, the smallest YOLOv5 model (YOLOv5s) has only 7 million parameters.</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 126.00px; height: 329.00px;"><img alt="" src="images/image5.png" style="width: 126.00px; height: 329.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 98.64px; height: 321.79px;"><img alt="" src="images/image6.png" style="width: 98.64px; height: 321.79px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c34"><span class="c1">Loss Function</span></p></td><td class="c20" colspan="2" rowspan="1"><p class="c9"><span class="c1">The YOLO loss function is composed of several components, including:</span></p><p class="c9 c21"><span class="c1"></span></p><p class="c9"><span class="c1">Localization loss: Measures the difference between the predicted bounding box coordinates and the ground truth bounding box coordinates.</span></p><p class="c9"><span class="c1">Confidence loss: Measures the difference between the predicted objectness score and the ground truth objectness score.</span></p><p class="c9"><span class="c1">Classification loss: Measures the difference between the predicted class probabilities and the ground truth class probabilities.</span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 644.93px; height: 163.87px;"><img alt="" src="images/image3.png" style="width: 644.93px; height: 163.87px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></td><td class="c12" colspan="2" rowspan="1"><p class="c9"><span class="c1">Both YOLOv3 and YOLOv5 use the same loss function.</span></p><p class="c9"><span class="c1">In YOLOv5, a different loss weighting scheme is used to improve accuracy. Specifically, the confidence loss term is weighted more heavily in YOLOv5 compared to YOLOv3. This is because the confidence loss term is more critical in object detection, as it determines which anchor box is responsible for detecting an object. By weighting the confidence loss term more heavily, YOLOv5 is able to improve the accuracy of object detection. Additionally, YOLOv5 also uses a different data augmentation scheme compared to YOLOv3, which helps to further improve accuracy.</span></p><p class="c23"><span class="c37">In YOLOv5, the GIoU loss function is used to optimize the model during training. The GIoU loss is a combination of the localization loss and the GIoU term, which measures the difference between the predicted bounding box and the ground truth bounding box in terms of size and location. By optimizing the GIoU loss, YOLOv5 is able to improve the accuracy and robustness of object detection, especially in cases where the objects have varying shapes and sizes.</span></p><p class="c23"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 696.09px; height: 176.87px;"><img alt="" src="images/image3.png" style="width: 696.09px; height: 176.87px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c37"></span></p><p class="c9 c21"><span class="c1"></span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c28">YOLOv8 uses </span><span class="c42">VFL Loss</span><span class="c18">&nbsp;as classification loss and DFL Loss+CIOU Loss as classification loss</span></p><p class="c9 c21"><span class="c18"></span></p><p class="c9"><span class="c18">It uses BCE loss as well</span></p></td></tr><tr class="c11"><td class="c2" colspan="5" rowspan="1"><p class="c34"><span class="c1">[5 pt ]Test video on the previous models.</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c9 c21"><span class="c1"></span></p></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c9"><span class="c1">Number of objects</span></p></td><td class="c20" colspan="2" rowspan="1"><p class="c9"><span class="c1">3069 objects (Darknet)</span></p><p class="c9"><span class="c1">2614 objects(ultralytics)</span></p></td><td class="c12" colspan="2" rowspan="1"><p class="c9"><span class="c1">2663 objects(ultralytics github repo)</span></p><p class="c9"><span class="c1">2583 objects(ultralytics python library)</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c1">1481 objects</span></p></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c9"><span class="c1">speed(Detection Time)</span></p><p class="c17"><span class="c1">Cpu or GPU</span></p></td><td class="c20" colspan="2" rowspan="1"><p class="c9"><span class="c1">11.288652658462524 seconds (Darknet)(GPU)</span></p><p class="c9"><span class="c1">509125.87ms (ultralytics)(CPU)</span></p></td><td class="c12" colspan="2" rowspan="1"><p class="c9"><span class="c1">255.8916175365448 seconds(ultralytics github repo)(CPU)</span></p><p class="c9"><span class="c1">Total time 123519.70ms (ultralytics python library)(CPU)</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c1">Very long time on cpu</span></p><p class="c9"><span class="c1">16.42167639732361seconds(GPU)</span></p></td></tr><tr class="c11"><td class="c26" colspan="6" rowspan="1"><p class="c9"><span class="c1">[2 pt] Bonus: Repeat the comparison for the next models.</span></p></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="2"><p class="c9"><span class="c1">Models</span></p></td><td class="c15" colspan="1" rowspan="2"><p class="c14"><span class="c1">yolov5n &nbsp;</span></p></td><td class="c3" colspan="1" rowspan="2"><p class="c14"><span class="c1">yolov5s</span></p></td><td class="c22" colspan="1" rowspan="2"><p class="c14"><span class="c1">yolov5m </span></p></td><td class="c5" colspan="1" rowspan="2"><p class="c14"><span class="c1">yolov5l &nbsp;</span></p></td><td class="c7" colspan="1" rowspan="2"><p class="c14"><span class="c1">yolov5x</span></p><p class="c9 c21"><span class="c1"></span></p></td></tr><tr class="c11"></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c9"><span class="c1">Deploying year</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c14"><span class="c1">2020</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c14"><span class="c1">2020</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c14"><span class="c1">2020</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c14"><span class="c1">2020</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c13"><span class="c1"></span></p><p class="c14"><span class="c1">2020</span></p></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c9"><span class="c1">Architecture</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c9 c21"><span class="c1"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1022.00px; height: 575.00px;"><img alt="" src="images/image4.png" style="width: 1022.00px; height: 575.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c1">Specifically, YOLOv5n is a medium-sized model with a backbone network consisting of 3 residual blocks, a neck network consisting of a single SPP module, and a detection head network consisting of a single YOLO layer. </span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c9 c21"><span class="c1"></span></p><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1022.00px; height: 575.00px;"><img alt="" src="images/image4.png" style="width: 1022.00px; height: 575.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c1">In comparison, YOLOv5s has a smaller backbone network with 2 residual blocks, a neck network with a single PAN module, and a detection head network with 3 YOLO layers.</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1022.00px; height: 575.00px;"><img alt="" src="images/image4.png" style="width: 1022.00px; height: 575.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c1">YOLOv5m has a larger backbone network with 4 residual blocks, a neck network with 2 PAN modules, and a detection head network with 3 YOLO layers. </span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 462.12px; height: 260.00px;"><img alt="" src="images/image4.png" style="width: 462.12px; height: 260.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c1">YOLOv5l has an even larger backbone network with 6 residual blocks, a neck network with 3 PAN modules, and a detection head network with 4 YOLO layers. </span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 1022.00px; height: 575.00px;"><img alt="" src="images/image4.png" style="width: 1022.00px; height: 575.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c9"><span class="c1">Finally, YOLOv5x has the largest backbone network with 8 residual blocks, a neck network with 4 PAN modules, and a detection head network with 4 YOLO layers.</span></p><p class="c9 c21"><span class="c1"></span></p></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c34"><span class="c1">Prediction scales</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c14"><span class="c1">The prediction scales for YOLOv5 can be represented as pixel sizes:</span></p><p class="c13"><span class="c1"></span></p><p class="c14"><span class="c1">128 pixels</span></p><p class="c14"><span class="c1">256 pixels</span></p><p class="c14"><span class="c1">512 pixels</span></p><p class="c14"><span class="c1">The prediction scales for YOLOv5 can also be represented as the number of feature maps:</span></p><p class="c13"><span class="c1"></span></p><p class="c14"><span class="c1">19 feature maps</span></p><p class="c14"><span class="c1">38 feature maps</span></p><p class="c14"><span class="c1">76 feature maps</span></p><p class="c14"><span class="c1">152 feature maps</span></p><p class="c14"><span class="c1">Both representations refer to the same concept of different scales used for object detection in YOLOv5.</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c14"><span class="c1">The prediction scales for YOLOv5 can be represented as pixel sizes:</span></p><p class="c13"><span class="c1"></span></p><p class="c14"><span class="c1">128 pixels</span></p><p class="c14"><span class="c1">256 pixels</span></p><p class="c14"><span class="c1">512 pixels</span></p><p class="c14"><span class="c1">The prediction scales for YOLOv5 can also be represented as the number of feature maps:</span></p><p class="c13"><span class="c1"></span></p><p class="c14"><span class="c1">19 feature maps</span></p><p class="c14"><span class="c1">38 feature maps</span></p><p class="c14"><span class="c1">76 feature maps</span></p><p class="c14"><span class="c1">152 feature maps</span></p><p class="c14"><span class="c1">Both representations refer to the same concept of different scales used for object detection in YOLOv5.</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c14"><span class="c1">The prediction scales for YOLOv5 can be represented as pixel sizes:</span></p><p class="c13"><span class="c1"></span></p><p class="c14"><span class="c1">128 pixels</span></p><p class="c14"><span class="c1">256 pixels</span></p><p class="c14"><span class="c1">512 pixels</span></p><p class="c14"><span class="c1">The prediction scales for YOLOv5 can also be represented as the number of feature maps:</span></p><p class="c13"><span class="c1"></span></p><p class="c14"><span class="c1">19 feature maps</span></p><p class="c14"><span class="c1">38 feature maps</span></p><p class="c14"><span class="c1">76 feature maps</span></p><p class="c14"><span class="c1">152 feature maps</span></p><p class="c14"><span class="c1">Both representations refer to the same concept of different scales used for object detection in YOLOv5.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c14"><span class="c1">The prediction scales for YOLOv5 can be represented as pixel sizes:</span></p><p class="c13"><span class="c1"></span></p><p class="c14"><span class="c1">128 pixels</span></p><p class="c14"><span class="c1">256 pixels</span></p><p class="c14"><span class="c1">512 pixels</span></p><p class="c14"><span class="c1">The prediction scales for YOLOv5 can also be represented as the number of feature maps:</span></p><p class="c13"><span class="c1"></span></p><p class="c14"><span class="c1">19 feature maps</span></p><p class="c14"><span class="c1">38 feature maps</span></p><p class="c14"><span class="c1">76 feature maps</span></p><p class="c14"><span class="c1">152 feature maps</span></p><p class="c14"><span class="c1">Both representations refer to the same concept of different scales used for object detection in YOLOv5.</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c14"><span class="c1">The prediction scales for YOLOv5 can be represented as pixel sizes:</span></p><p class="c13"><span class="c1"></span></p><p class="c14"><span class="c1">128 pixels</span></p><p class="c14"><span class="c1">256 pixels</span></p><p class="c14"><span class="c1">512 pixels</span></p><p class="c14"><span class="c1">The prediction scales for YOLOv5 can also be represented as the number of feature maps:</span></p><p class="c13"><span class="c1"></span></p><p class="c14"><span class="c1">19 feature maps</span></p><p class="c14"><span class="c1">38 feature maps</span></p><p class="c14"><span class="c1">76 feature maps</span></p><p class="c14"><span class="c1">152 feature maps</span></p><p class="c14"><span class="c1">Both representations refer to the same concept of different scales used for object detection in YOLOv5.</span></p></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c34"><span class="c1">&nbsp;Number of Anchors</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c9"><span class="c1">YOLOv5 uses three to five anchors per scale. This reduces the number of parameters and helps to improve speed and accuracy.</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c9"><span class="c1">YOLOv5 uses three to five anchors per scale. This reduces the number of parameters and helps to improve speed and accuracy.</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c9"><span class="c1">YOLOv5 uses three to five anchors per scale. This reduces the number of parameters and helps to improve speed and accuracy.</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c9"><span class="c1">YOLOv5 uses three to five anchors per scale. This reduces the number of parameters and helps to improve speed and accuracy.</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c9"><span class="c1">YOLOv5 uses three to five anchors per scale. This reduces the number of parameters and helps to improve speed and accuracy.</span></p></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c34"><span class="c30">Number of </span><span class="c30 c38">params</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c14"><span class="c1">1867405 parameters</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c14"><span class="c1">7225885 parameters</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c14"><span class="c1">21172173 parameters</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c14"><span class="c1">46533693 parameters</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c14"><span class="c1">86705005 parameters</span></p></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c34"><span class="c1">Loss Function</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c14"><span class="c1">Generalized IoU (GIoU)</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c14"><span class="c1">Generalized IoU (GIoU)</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c14"><span class="c1">Generalized IoU (GIoU)</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c14"><span class="c1">Generalized IoU (GIoU)</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c14"><span class="c1">Generalized IoU (GIoU)</span></p></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c9"><span class="c1">Number of objects</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c14"><span class="c1">1906 objects</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c14"><span class="c1">2261 objects</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c14"><span class="c1">2529 objects</span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c14"><span class="c1">2663 objects</span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c14"><span class="c1">2613 objects</span></p><p class="c13"><span class="c1"></span></p></td></tr><tr class="c11"><td class="c6" colspan="1" rowspan="1"><p class="c38 c39"><span class="c1">speed(Detection Time)</span></p><p class="c27"><span class="c1">Cpu</span></p></td><td class="c15" colspan="1" rowspan="1"><p class="c14"><span class="c1">22.93666362762451 seconds</span></p><p class="c14"><span class="c1">Speed (Detection Time): 62 FPS on Tesla V100 GPU</span></p><p class="c13"><span class="c1"></span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c14"><span class="c1">45.31887912750244 seconds</span></p><p class="c14"><span class="c1">Speed (Detection Time): 140 FPS on Tesla P100 GPU</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c14"><span class="c1">132.85901141166687 seconds</span></p><p class="c14"><span class="c1">Speed (Detection Time): 60 FPS on Tesla V100 GPU</span></p><p class="c13"><span class="c1"></span></p></td><td class="c5" colspan="1" rowspan="1"><p class="c14"><span class="c1">255.8916175365448 seconds</span></p><p class="c14"><span class="c1">Speed (Detection Time): 40 FPS on Tesla V100 GPU</span></p><p class="c13"><span class="c1"></span></p></td><td class="c7" colspan="1" rowspan="1"><p class="c14"><span class="c1">452.72636580467224 seconds</span></p><p class="c14"><span class="c1">Speed (Detection Time): 14 FPS on Tesla V100 GPU</span></p></td></tr></table><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c0"><span class="c1"></span></p><p class="c16"><span class="c35">Architectures</span></p><p class="c0"><span class="c1"></span></p><p class="c16"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 512.00px;"><img alt="" src="images/image10.png" style="width: 624.00px; height: 512.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c24"><span class="c19">Yolov5</span></p><p class="c16"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 661.35px; height: 493.70px;"><img alt="" src="images/image8.png" style="width: 661.35px; height: 493.70px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c16"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 947.58px; height: 992.15px;"><img alt="" src="images/image7.png" style="width: 947.58px; height: 992.15px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p></body></html>